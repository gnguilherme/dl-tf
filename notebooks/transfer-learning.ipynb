{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import tensorflow as tf\r\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\r\n",
    "mnet = MobileNetV2(input_shape=(224, 224, 3), include_top=False, alpha=1)  # para controlar o tamanho da rede, pode mudar o parâmetro alpha\r\n",
    "mnet"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import tensorflow_datasets as tfds\r\n",
    "tfds.disable_progress_bar()\r\n",
    "\r\n",
    "dataset, metadata = tfds.load('beans', as_supervised=True, with_info=True)\r\n",
    "train_dataset = dataset['train']\r\n",
    "validation_dataset = dataset['validation']\r\n",
    "test_dataset = dataset['test']\r\n",
    "\r\n",
    "class_names = metadata.features['label'].names\r\n",
    "print(\"Class names: {}\".format(class_names))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def normalize(images, labels):\r\n",
    "    images = tf.cast(images, tf.float32)\r\n",
    "    images /= 255\r\n",
    "    return images, labels\r\n",
    "\r\n",
    "def resize(images, labels):\r\n",
    "    images = tf.image.resize(images, size=(224, 224))\r\n",
    "    labels = tf.one_hot(labels, depth=len(class_names))\r\n",
    "\r\n",
    "    return images, labels\r\n",
    "\r\n",
    "def transform_images(images, labels):\r\n",
    "    images, labels = resize(images, labels)\r\n",
    "    images, labels = normalize(images, labels)\r\n",
    "\r\n",
    "    return images, labels\r\n",
    "\r\n",
    "train_dataset =  train_dataset.map(transform_images)\r\n",
    "validation_dataset =  validation_dataset.map(transform_images)\r\n",
    "test_dataset =  test_dataset.map(transform_images)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for image, labels in train_dataset.take(1):\r\n",
    "    break\r\n",
    "pred = mnet(tf.expand_dims(image, 0))\r\n",
    "pred.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\r\n",
    "\r\n",
    "mnet.trainable = False\r\n",
    "gap = GlobalAveragePooling2D()\r\n",
    "dropout = Dropout(rate=0.25)\r\n",
    "fc1 = Dense(units=len(class_names), activation=tf.nn.softmax)\r\n",
    "layers =[\r\n",
    "    mnet,  # feature extractor\r\n",
    "    gap,  # no lugar do flatten, utilizamos GAP\r\n",
    "    dropout,\r\n",
    "    fc1,\r\n",
    "]\r\n",
    "model = tf.keras.models.Sequential(layers)\r\n",
    "model.summary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.compile(\r\n",
    "    optimizer='adam',\r\n",
    "    loss='categorical_crossentropy',\r\n",
    "    metrics=['accuracy']\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\r\n",
    "\r\n",
    "BATCH_SIZE = 32\r\n",
    "\r\n",
    "num_train_examples = metadata.splits['train'].num_examples\r\n",
    "num_validation_examples = metadata.splits['validation'].num_examples\r\n",
    "num_test_examples = metadata.splits['test'].num_examples\r\n",
    "\r\n",
    "train_dataset = train_dataset \\\r\n",
    "    .cache() \\\r\n",
    "    .repeat() \\\r\n",
    "    .shuffle(num_train_examples) \\\r\n",
    "    .batch(BATCH_SIZE) \\\r\n",
    "    .prefetch(tf.data.AUTOTUNE)\r\n",
    "\r\n",
    "validation_dataset = validation_dataset \\\r\n",
    "    .cache() \\\r\n",
    "    .repeat() \\\r\n",
    "    .shuffle(num_validation_examples) \\\r\n",
    "    .batch(BATCH_SIZE) \\\r\n",
    "    .prefetch(tf.data.AUTOTUNE)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "history = model.fit(\r\n",
    "    train_dataset, epochs=10,\r\n",
    "    steps_per_epoch=np.ceil(num_train_examples / BATCH_SIZE),  # esses passos só servem porque fizemos o .repeat() no dataset\r\n",
    "    validation_data=validation_dataset,\r\n",
    "    validation_steps=np.ceil(num_validation_examples / BATCH_SIZE),  # esses passos só servem porque fizemos o .repeat() no dataset\r\n",
    "    workers=-1,\r\n",
    ")\r\n",
    "\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 6))\r\n",
    "ax[0].plot(history.history['loss'], label='loss')\r\n",
    "ax[0].plot(history.history['val_loss'], label='val loss')\r\n",
    "ax[0].legend(frameon=False)\r\n",
    "ax[1].plot(history.history['accuracy'], label='acc')\r\n",
    "ax[1].plot(history.history['val_accuracy'], label='val acc')\r\n",
    "ax[1].legend(frameon=False)\r\n",
    "fig.suptitle(\"Training loss and acc\", fontweight='bold', fontsize=14);"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_dataset = test_dataset \\\r\n",
    "    .cache() \\\r\n",
    "    .batch(BATCH_SIZE)\r\n",
    "model.evaluate(test_dataset)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('tf': venv)"
  },
  "interpreter": {
   "hash": "a8da867777bda7a36ba79ef652ca0450f4f134accc992c4581dd001533c4e60a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}